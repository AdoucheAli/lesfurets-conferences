<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <title>Apache Spark : Hands-on et use cases pour développeurs Java</title>
        <meta name="description" content="Continuous delivery chez LesFurets.com">
        <meta name="author" content="Alexandre DuBreuil">
        <meta name="apple-mobile-web-app-capable" content="yes" />
        <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent" />
        <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">
        <link rel="stylesheet" href="../bower_components/reveal.js/css/reveal.css">
        <link rel="stylesheet" href="../bower_components/reveal.js/lib/css/zenburn.css">
        <link rel="stylesheet" href="../css/lesfurets-theme.css" id="theme">
        <link rel="stylesheet" href="../css/git-octopus-theme.css" id="theme">
        <script>
if( window.location.search.match( /print-pdf/gi ) ) {
    var link = document.createElement( 'link' );
    link.rel = 'stylesheet';
    link.type = 'text/css';
    link.href = '../css/print/pdf.css';
    document.getElementsByTagName( 'head' )[0].appendChild( link );
}
        </script>
        <!--[if lt IE 9]><script src="../bower_components/reveal.js/lib/js/html5shiv.js"></script><![endif]-->
    </head>
    <body>
        <div id="footer" class="footer show">
            <a href="https://www.lesfurets.com" target="_blank">
                <img class="logo" src="../img/logo_lesfurets_885x128_no_back.png">
            </a>
            <a class="github" href="https://github.com/lesfurets" target="_blank">https://github.com/lesfurets</a>
            <a class="twitter" href="https://twitter.com/BeastieFurets" target="_blank">@BeastieFurets</a>
            <a class="github" href="https://github.com/dubreuia" target="_blank">https://github.com/dubreuia</a>
            <a style="margin-right: 10px;" class="twitter" href="https://twitter.com/dubreuia" target="_blank">@dubreuia</a>
            <img style="height:40px;vertical-align:middle;padding:0 10px 0 25px" src="../img/TODO.png">
            <span style="font-family:arial;font-weight:bold;font-size:25px;vertical-align:middle">SNOWCAMP</span>
        </div>
        <div class="reveal">
            <div class="slides">

                <!-- SECTION - INTRO -->

                <section class="flushright" data-background="../img/nwx/lesfurets-background-black-01.jpg">
                    <h1>Apache Spark</h1>
                    <h2>hands-on et use cases pour développeurs Java</h2>
                    <h3>Alexandre DuBreuil</h3>
                </section>

                <section class="flushleft" data-background="../img/nwx/lesfurets-background-black-01.jpg">
                    <h2>Alexandre DuBreuil</h2>
                    <ul class="flushright nodisc">
                        <li>
                            <a style="color:white" href="https://twitter.com/dubreuia">https://twitter.com/dubreuia</a>
                        </li>
                        <li>
                            <a style="color:white" href="https://github.com/dubreuia">https://github.com/dubreuia</a>
                        </li>
                    </ul>
                </section>

                <!-- SECTION - INTRODUCTION -->

                <section>

                    <section class="flushleft" data-state="background-pipeline">
                        <h2 style="color:white">Introduction</h2>
                    </section>

                    <section class="flushleft" data-background="#222">
                        <h3>Definition</h3>
                        <p class="">Apache Spark est un système de calcul distribué général haute performance</p>
                        <p class="fragment">Il propose des API haut niveau en Java, Scala, Python et R et contient un engin d'optimisation générique</p>
                        <p class="fragment">Il contient plusieurs outils tels que Spark SQL pour la gestion de donnée en SQL, MLlib pour le machine learning, GraphX pour le processing de graph et Spark Streaming pour du micro-batching</p>
                    </section>

                    <section class="flushleft" data-background="#222">
                        <h3>Composants</h3>
                        <!-- https://image.slidesharecdn.com/sparksolrrev-timpotter-151021184307-lva1-app6892/95/solr-and-spark-for-realtime-big-data-analytics-presented-by-tim-potter-lucidworks-5-638.jpg -->
                        <img style="width:66%" src="../img/spark/spark-components.jpg">
                    </section>

                    <section class="flushleft" data-background="#222">
                        <h3>Architecture</h3>
                        <!-- https://spark.apache.org/docs/latest/cluster-overview.html -->
                        <img style="width:66%" src="../img/spark/cluster-overview.png">
                    </section>

                    <section class="flushleft" data-background="#222">
                        <h3>Chez LesFurets.com</h3>
                        <p>Glossary + image on https://spark.apache.org/docs/latest/cluster-overview.html<p>
                    </section>

                </section>

                <!-- SECTION - APACHE SPARK -->

                <section>

                    <section class="flushleft" data-state="background-pipeline">
                        <h2 style="color:white">Apache Spark</h2>
                    </section>

                    <section class="flushleft" data-background="#222">
                        <h3>SparkSession</h3>
                        <p>Le point d'entrée de toute les fonctionnalités spark est la classe <code>SparkSession</code></p>
                        <pre><code class="java" data-trim>
SparkSession spark = SparkSession.builder()
        .appName("Salut Snowcamp")
        .config("spark.some.config.option",
                "some-value")
        .getOrCreate();
                        </code></pre>
                    </section>

                    <section class="flushleft" data-background="#222">
                        <h3>DataFrame</h3>
                        <p>Un DataFrame est une collection distribuée de data organisée en colonnes nommées et typées. Les exécutions basées sur cette API vont être optimisée par Catalyst.</p>
                        <p>A partir de notre <code>SparkSession</code> on récupère un <code>DataSet&lt;Row&gt;</code> (soit un DataSet non-typé, appelé DataFrame).</p>
                        <pre><code class="java" data-trim>
// Lecture d'un fichier data.csv avec inférence de schéma
Dataset&lt;Row&gt; data = spark.read()
        .option("inferSchema", true)
        .csv("data.csv");
                        </code></pre>
                        <!-- In Spark, a DataFrame is a distributed collection of data organized into named columns. Users can use DataFrame API to perform various relational operations on both external data sources and Spark’s built-in distributed collections without providing specific procedures for processing data. Also, programs based on DataFrame API will be automatically optimized by Spark’s built-in optimizer, Catalyst -->
                    </section>

                    <section class="flushleft" data-background="#222">
                        <h3>Les DataFrames ont un schéma</h3>
                        <pre><code class="java" data-trim>
data.printSchema();
                        </code></pre>
                        <pre><code class="java" data-trim>
root
 |-- uid: string (nullable = true)
 |-- email_hash: integer (nullable = true)
 |-- date: timestamp (nullable = true)
 |-- heure: string (nullable = true)
 |-- module: string (nullable = true)
                        </code></pre>
                    </section>

                    <section class="flushleft" data-background="#222">
                        <h3>DateFrame API</h3>
                        <img style="width:66%" src="../img/spark/dataframe-diagram.png">
                        <!-- TODO why reduceByKey missing ? slower than groupByKey... check instapaper article -->
                        <!-- 
Why Use DataFrames instead of RDDs?
For new users familiar with data frames in other programming languages, this API should make them feel at home
For existing Spark users, the API will make Spark easier to program than using RDDs
For both sets of users, DataFrames will improve performance through intelligent optimizations and code-generation 
                        -->
                    </section>

                    <section class="flushleft" data-background="#222">
                        <h3>DataSet</h3>
                        <p>On récupère un <code>DataSet</code> tel quel, ou on converti un <code>DataFrame</code> en le typant.</p>
                        <p>Soit <code>Question</code> un bean java qui correspond à une question du formulaire LesFurets</p>
                        <p>%md Dataset strong typing is "virtual" -- it's just a view that can be applied when you want it:</p>
                        <pre><code class="java" data-trim>
// Convertion du Dataset&lt;Row&gt; en Dataset&lt;Question&gt;
Dataset&lt;Question&gt; domainData = data
        .as(Encoders.bean(Question.class))
                        </code></pre>
                    </section>

                    <section class="flushleft" data-background="#222">
                        <h3>RDD / DataFrames / DataSets</h3>
                        <p>À partir de Spark 2.0, SparkSQL, DataFrames and DataSets représentent le même composant</p>
                        <!-- https://i.stack.imgur.com/3rF6p.png -->
                        <img style="width:66%" src="../img/spark/rdd-dataframe-dataset.png">
                        <!--
DataFrames
The preferred abstraction in Spark (introduced in 1.3)
Strongly typed collection of distributed elements
Built on Resilient Distributed Datasets
Immutable once constructed
Track lineage information to efficiently recompute lost data
Enable operations on collection of elements in parallel

You construct DataFrames
by parallelizing existing collections (e.g., Pandas DataFrames)
by transforming an existing DataFrame
from files in HDFS, Hive tables, or any other storage system (e.g., Parquet in S3)

Modern SparkSQL and DataFrames represent the same thing: a new language-independent, high-performance query engine implementation
SparkSQL/DataFrames is different from, and replaces, a variety of older approaches including Shark, Hive-on-Spark, and SchemaRDD
Although Spark contains its own data processing engine, it can integrate closely with a Hive metastore

DataFrame/DataSet API and SQL (typically via the HiveQL parser) are equivalent ways to do the same tasks.
SQL serves many general purposes, including analytic work via BI tools (over JDBC and the Thriftserver)
DataFrames offer more programmatic control and an API familiar to users of Pandas or R
DataSets are a generalization of DataFrames and the underlying engine, to support more data types and stronger type enforcement
                        -->
                    </section>

                    <section class="flushleft" data-background="#222">
                        <h3>Resilient Distributed Datasets (RDDs)</h3>
                        <p>C'est la plomberie interne de spark : on n'y touche pas, sauf si vous avez besoin / savez exactement ce que vous faites</p>
                        <!-- 
                        -->
                        <pre><code class="java" data-trim>
// Récupération du RDD sous-jacent au dataset
RDD&lt;Question&gt; rdd = domainData.rdd();
// API Java du RDD
JavaRDD&lt;Question&gt; javaRDD = domainData.javaRDD();
                        </code></pre>
                    </section>


                    <section class="flushleft" data-background="#222">
                        <h3>Catalyst / whole-stage codegen</h3>
                        <p>TODO whole-stage codegen example</p>
                        <p>TODO link spark summit conference</p>
                        <!-- 
TODO
                        -->
                    </section>

                    <section class="flushleft" data-background="#222">
                        <h3>Performance</h3>
                        <!-- https://www.ardentex.com/publications/RDDs-DataFrames-and-Datasets-in-Apache-Spark/images/dataframe-performance.png -->
                        <img style="width:66%" src="../img/spark/dataframe-performance.png">
                        <!-- 
https://databricks.com/blog/2015/02/17/introducing-dataframes-in-spark-for-large-scale-data-science.html
https://gist.github.com/rxin/c1592c133e4bccf515dd
                        -->
                    </section>

                    <section class="flushleft" data-background="#222">
                        <h3>Data sources</h3>
                        <img style="width:66%" src="../img/spark/dataframe-sources.png">
                        <!-- 
                        -->
                    </section>

                    <section class="flushleft" data-background="#222">
                        <h3>Data store (disk / parquet)</h3>
                        <p>code</p>
                        <p>https://academy.datastax.com/resources/getting-started-apache-spark-and-cassandra?unit=getting-started-apache-spark-and-cassandra<p>
                        <!-- 
In the previous example, we created DataFrames from Parquet and JSON data.
A Parquet table has a schema (column names and types) that Spark can use. Parquet also allows Spark to be efficient about how it pares down data.
Spark can infer a Schema from a JSON file.
                        -->
                    </section>

                    <section class="flushleft" data-background="#222">
                        <h3>Data store (Cassandra)</h3>
                        <p>code</p>
                        <p>https://academy.datastax.com/resources/getting-started-apache-spark-and-cassandra?unit=getting-started-apache-spark-and-cassandra<p>
                    </section>

                </section>

                <!-- SECTION - ENVIRONNEMENT -->

                <section>

                    <section class="flushleft" data-state="background-pipeline">
                        <h2 style="color:white">Environnement pour développeur</h2>
                    </section>

                    <section class="flushleft" data-background="#222">
                        <h3>Notebook</h3>
                        <!-- our -->
                        <img style="width:66%" src="../img/spark/databricks-notebook.png">
                    </section>

                    <section class="flushleft" data-background="#222">
                        <h3>Industrialisation</h3>
                        <p>Les notebooks Java n'existent pas, entre autre parce qu'on n'a pas (encore) de REPL en Java</p>
                        <p>Les notebooks peuvent être pratiques, mais on a besoin de gérer le versionnement du code source, la compilation, les tests unitaires, l'analyse statique, la mise en production.</p>
                    </section>

                    <section class="flushleft" data-background="#222">
                        <h3>Exécution locale</h3>
                        <p>Avoir un cluster en local : facile, c'est ce que je fais pour cette démo (mode standalone)</p>
                        <p>Il y a plusieurs possibilités, soit standalone, soit standalone single-jvm (aussi appellé "local"). Ce dernier permet d'éviter d'avoir à gérer un jar à distribuer entre les instances des exécuteurs</p>
                    </section>

                    <section class="flushleft" data-background="#222">
                        <h3>local[*]</h3>
                        <p>Spark local (pseudo-cluster): You can run Spark in local mode. In this non-distributed single-JVM deployment mode, Spark spawns all the execution components - driver, executor, backend, and master - in the same single JVM. The default parallelism is the number of threads as specified in the master URL. This is the only mode where a driver is used for execution.</p>
                    </section>

                    <section class="flushleft" data-background="#222">
                        <h3>Maven</h3>
                        <p>Les dépendances dans votre code peuvent être gérés par Maven, les packages Spark sont dans Maven Central.</p>
                        <p>Ici, on utilise la version 2.0.2 de spark-core qui a été compilée avec Scala 2.11 (important que cette version corresponde aux versions installées sur les machines d'exécution, parce que les jar runtime sont partagés)</p>
                        <pre><code class="xml" data-trim>
<dependency>
  <groupId>org.apache.spark</groupId>
  <artifactId>spark-core_2.11</artifactId>
  <version>2.0.2</version>
</dependency>
                        </code></pre>
                    </section>

                    <section class="flushleft" data-background="#222">
                        <h3>IDE</h3>
                        <p>Dans n'importe quel IDE, il suffit de lancer un cluster local et d'exécuter un main pour utiliser spark</p>
                        <p>DEMO TIME</p>
                        <p>voir com.lesfurets.slides.IDE</p>
                    </section>

                    <section class="flushleft" data-background="#222">
                        <h3>TU</h3>
                        <p>Spark est facilement testable, il suffit de créer des Dataframe ou Dataset basé sur du contenu de test. Il est aussi possible de tester SparkSql en créant à la volée les tables nécessaires.</p>
                        <p>DEMO TIME</p>
                        <p>voir com.lesfurets.slides.TestUnitaires</p>
                    </section>

                    <section class="flushleft" data-background="#222">
                        <h3>JUnit5</h3>
                        <p>Instancier un SparkContext global avec @BeforeAll et @AfterAll</p>
                        <p>@Tag pour tagger des tests spark d'integration</p>
                        <p>org.junit.jupiter.api.Assumptions pour activer des tests en local</p>
                    </section>

                    <section class="flushleft" data-background="#222">
                        <h3>CI</h3>
                        <p>todo jenkins</p>
                        <p>DEMO TIME</p>
                    </section>

                    <section class="flushleft" data-background="#222">
                        <h3>vs Scala</h3>
                        <p>todo api diff</p>
                    </section>

                </section>

                <!-- SECTION - USE CASE -->

                <section>

                    <section class="flushleft" data-state="background-pipeline">
                        <h2 style="color:white">Cas d'utilisation</h2>
                        <h3 style="color:white">Todo decision tree ?</h3>
                    </section>

                </section>

                <!-- SECTION - END -->

                <section>

                    <section class="flushleft" data-background="../img/nwx/lesfurets-background-black-01.jpg">
                        <h2 style="color:white">END</h2>
                    </section>

                </section>

            </div>
        </div>
        <script src="../bower_components/reveal.js/lib/js/head.min.js"></script>
        <script src="../bower_components/reveal.js/js/reveal.js"></script>
        <script>
// Full list of configuration options available here:
// https://github.com/hakimel/reveal.js#configuration
Reveal.initialize({
    controls: true,
    progress: true,
    history: true,
    center: true,
    embedded: true,

    // Hack my remote because it send 5 events
    keyboard: {
        // Tab : nop
        9: null,
        // Page down : previous slide
        33: function() { 
            if (!window.animate) {
                window.animate = true;
                Reveal.left();
                setTimeout(function() { window.animate = false; }, 1000);
            }
        },
        // Page up : next slide
        34: function() { 
            if (!window.animate) {
                window.animate = true;
                Reveal.right(); 
                setTimeout(function() { window.animate = false; }, 1000);
            }
        }
    },


    //theme: 'lesfurets', // available themes are in /css/theme
    transition: Reveal.getQueryHash().transition || 'linear', // default/cube/page/concave/zoom/linear/fade/none

    // Parallax scrolling
    // parallaxBackgroundImage: 'https://s3.amazonaws.com/hakim-static/reveal-js/reveal-parallax-1.jpg',
    // parallaxBackgroundSize: '2100px 900px',

    // Optional libraries used to extend on reveal.js
    dependencies: [
    { src: '../bower_components/reveal.js/lib/js/classList.js', condition: function() { return !document.body.classList; } },
    { src: '../bower_components/reveal.js/plugin/markdown/marked.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
    { src: '../bower_components/reveal.js/plugin/markdown/markdown.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
    { src: '../bower_components/reveal.js/plugin/highlight/highlight.js', async: true, callback: function() { hljs.initHighlightingOnLoad(); } },
    { src: '../bower_components/reveal.js/plugin/zoom-js/zoom.js', async: true, condition: function() { return !!document.body.classList; } },
    { src: '../bower_components/reveal.js/plugin/notes/notes.js', async: true, condition: function() { return !!document.body.classList; } }
    ]
});
        </script>
        <script src="../js/lesfurets-theme.js" async></script>
    </body>
</html>

